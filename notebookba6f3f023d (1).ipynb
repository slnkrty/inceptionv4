{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":1658282,"sourceType":"datasetVersion","datasetId":902873},{"sourceId":8450648,"sourceType":"datasetVersion","datasetId":5036090}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Contains common code shared by all inception models.\n\nUsage of arg scope:\n  with slim.arg_scope(inception_arg_scope()):\n    logits, end_points = inception.inception_v3(images, num_classes,\n                                                is_training=is_training)\n\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\nimport tf_slim as slim\n\n\ndef inception_arg_scope(\n    weight_decay=0.00004,\n    use_batch_norm=True,\n    batch_norm_decay=0.9997,\n    batch_norm_epsilon=0.001,\n    activation_fn=tf.nn.relu,\n    batch_norm_updates_collections=tf.GraphKeys.UPDATE_OPS,\n    batch_norm_scale=False):\n  \"\"\"Defines the default arg scope for inception models.\n\n  Args:\n    weight_decay: The weight decay to use for regularizing the model.\n    use_batch_norm: \"If `True`, batch_norm is applied after each convolution.\n    batch_norm_decay: Decay for batch norm moving average.\n    batch_norm_epsilon: Small float added to variance to avoid dividing by zero\n      in batch norm.\n    activation_fn: Activation function for conv2d.\n    batch_norm_updates_collections: Collection for the update ops for\n      batch norm.\n    batch_norm_scale: If True, uses an explicit `gamma` multiplier to scale the\n      activations in the batch normalization layer.\n\n  Returns:\n    An `arg_scope` to use for the inception models.\n  \"\"\"\n  batch_norm_params = {\n      # Decay for the moving averages.\n      'decay': batch_norm_decay,\n      # epsilon to prevent 0s in variance.\n      'epsilon': batch_norm_epsilon,\n      # collection containing update_ops.\n      'updates_collections': batch_norm_updates_collections,\n      # use fused batch norm if possible.\n      'fused': None,\n      'scale': batch_norm_scale,\n  }\n  if use_batch_norm:\n    normalizer_fn = slim.batch_norm\n    normalizer_params = batch_norm_params\n  else:\n    normalizer_fn = None\n    normalizer_params = {}\n  # Set weight_decay for weights in Conv and FC layers.\n  with slim.arg_scope([slim.conv2d, slim.fully_connected],\n                      weights_regularizer=slim.l2_regularizer(weight_decay)):\n    with slim.arg_scope(\n        [slim.conv2d],\n        weights_initializer=slim.variance_scaling_initializer(),\n        activation_fn=activation_fn,\n        normalizer_fn=normalizer_fn,\n        normalizer_params=normalizer_params) as sc:\n      return sc","metadata":{"execution":{"iopub.status.busy":"2024-05-18T14:26:56.627584Z","iopub.execute_input":"2024-05-18T14:26:56.628373Z","iopub.status.idle":"2024-05-18T14:26:56.639961Z","shell.execute_reply.started":"2024-05-18T14:26:56.628341Z","shell.execute_reply":"2024-05-18T14:26:56.638820Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n# http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Contains the definition of the Inception V4 architecture.\n\nAs described in http://arxiv.org/abs/1602.07261.\n\n  Inception-v4, Inception-ResNet and the Impact of Residual Connections\n    on Learning\n  Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, Alex Alemi\n\"\"\"\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport tensorflow.compat.v1 as tf\nimport tf_slim as slim\n\n\ndef block_inception_a(inputs, scope=None, reuse=None):\n  \"\"\"Builds Inception-A block for Inception v4 network.\"\"\"\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding='SAME'):\n    with tf.compat.v1.variable_scope(\n        scope, 'BlockInceptionA', [inputs], reuse=reuse):\n      with tf.compat.v1.variable_scope('Branch_0'):\n        branch_0 = slim.conv2d(inputs, 96, [1, 1], scope='Conv2d_0a_1x1')\n      with tf.compat.v1.variable_scope('Branch_1'):\n        branch_1 = slim.conv2d(inputs, 64, [1, 1], scope='Conv2d_0a_1x1')\n        branch_1 = slim.conv2d(branch_1, 96, [3, 3], scope='Conv2d_0b_3x3')\n      with tf.compat.v1.variable_scope('Branch_2'):\n        branch_2 = slim.conv2d(inputs, 64, [1, 1], scope='Conv2d_0a_1x1')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0b_3x3')\n        branch_2 = slim.conv2d(branch_2, 96, [3, 3], scope='Conv2d_0c_3x3')\n      with tf.compat.v1.variable_scope('Branch_3'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')\n        branch_3 = slim.conv2d(branch_3, 96, [1, 1], scope='Conv2d_0b_1x1')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_a(inputs, scope=None, reuse=None):\n  \"\"\"Builds Reduction-A block for Inception v4 network.\"\"\"\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding='SAME'):\n    with tf.compat.v1.variable_scope(\n        scope, 'BlockReductionA', [inputs], reuse=reuse):\n      with tf.compat.v1.variable_scope('Branch_0'):\n        branch_0 = slim.conv2d(inputs, 384, [3, 3], stride=2, padding='VALID',\n                               scope='Conv2d_1a_3x3')\n      with tf.compat.v1.variable_scope('Branch_1'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')\n        branch_1 = slim.conv2d(branch_1, 224, [3, 3], scope='Conv2d_0b_3x3')\n        branch_1 = slim.conv2d(branch_1, 256, [3, 3], stride=2,\n                               padding='VALID', scope='Conv2d_1a_3x3')\n      with tf.compat.v1.variable_scope('Branch_2'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID',\n                                   scope='MaxPool_1a_3x3')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_b(inputs, scope=None, reuse=None):\n  \"\"\"Builds Inception-B block for Inception v4 network.\"\"\"\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding='SAME'):\n    with tf.compat.v1.variable_scope(\n        scope, 'BlockInceptionB', [inputs], reuse=reuse):\n      with tf.compat.v1.variable_scope('Branch_0'):\n        branch_0 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')\n      with tf.compat.v1.variable_scope('Branch_1'):\n        branch_1 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')\n        branch_1 = slim.conv2d(branch_1, 224, [1, 7], scope='Conv2d_0b_1x7')\n        branch_1 = slim.conv2d(branch_1, 256, [7, 1], scope='Conv2d_0c_7x1')\n      with tf.compat.v1.variable_scope('Branch_2'):\n        branch_2 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')\n        branch_2 = slim.conv2d(branch_2, 192, [7, 1], scope='Conv2d_0b_7x1')\n        branch_2 = slim.conv2d(branch_2, 224, [1, 7], scope='Conv2d_0c_1x7')\n        branch_2 = slim.conv2d(branch_2, 224, [7, 1], scope='Conv2d_0d_7x1')\n        branch_2 = slim.conv2d(branch_2, 256, [1, 7], scope='Conv2d_0e_1x7')\n      with tf.compat.v1.variable_scope('Branch_3'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')\n        branch_3 = slim.conv2d(branch_3, 128, [1, 1], scope='Conv2d_0b_1x1')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef block_reduction_b(inputs, scope=None, reuse=None):\n  \"\"\"Builds Reduction-B block for Inception v4 network.\"\"\"\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding='SAME'):\n    with tf.compat.v1.variable_scope(\n        scope, 'BlockReductionB', [inputs], reuse=reuse):\n      with tf.compat.v1.variable_scope('Branch_0'):\n        branch_0 = slim.conv2d(inputs, 192, [1, 1], scope='Conv2d_0a_1x1')\n        branch_0 = slim.conv2d(branch_0, 192, [3, 3], stride=2,\n                               padding='VALID', scope='Conv2d_1a_3x3')\n      with tf.compat.v1.variable_scope('Branch_1'):\n        branch_1 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')\n        branch_1 = slim.conv2d(branch_1, 256, [1, 7], scope='Conv2d_0b_1x7')\n        branch_1 = slim.conv2d(branch_1, 320, [7, 1], scope='Conv2d_0c_7x1')\n        branch_1 = slim.conv2d(branch_1, 320, [3, 3], stride=2,\n                               padding='VALID', scope='Conv2d_1a_3x3')\n      with tf.compat.v1.variable_scope('Branch_2'):\n        branch_2 = slim.max_pool2d(inputs, [3, 3], stride=2, padding='VALID',\n                                   scope='MaxPool_1a_3x3')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2])\n\n\ndef block_inception_c(inputs, scope=None, reuse=None):\n  \"\"\"Builds Inception-C block for Inception v4 network.\"\"\"\n  # By default use stride=1 and SAME padding\n  with slim.arg_scope([slim.conv2d, slim.avg_pool2d, slim.max_pool2d],\n                      stride=1, padding='SAME'):\n    with tf.compat.v1.variable_scope(\n        scope, 'BlockInceptionC', [inputs], reuse=reuse):\n      with tf.compat.v1.variable_scope('Branch_0'):\n        branch_0 = slim.conv2d(inputs, 256, [1, 1], scope='Conv2d_0a_1x1')\n      with tf.compat.v1.variable_scope('Branch_1'):\n        branch_1 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')\n        branch_1 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_1, 256, [1, 3], scope='Conv2d_0b_1x3'),\n            slim.conv2d(branch_1, 256, [3, 1], scope='Conv2d_0c_3x1')])\n      with tf.compat.v1.variable_scope('Branch_2'):\n        branch_2 = slim.conv2d(inputs, 384, [1, 1], scope='Conv2d_0a_1x1')\n        branch_2 = slim.conv2d(branch_2, 448, [3, 1], scope='Conv2d_0b_3x1')\n        branch_2 = slim.conv2d(branch_2, 512, [1, 3], scope='Conv2d_0c_1x3')\n        branch_2 = tf.concat(axis=3, values=[\n            slim.conv2d(branch_2, 256, [1, 3], scope='Conv2d_0d_1x3'),\n            slim.conv2d(branch_2, 256, [3, 1], scope='Conv2d_0e_3x1')])\n      with tf.compat.v1.variable_scope('Branch_3'):\n        branch_3 = slim.avg_pool2d(inputs, [3, 3], scope='AvgPool_0a_3x3')\n        branch_3 = slim.conv2d(branch_3, 256, [1, 1], scope='Conv2d_0b_1x1')\n      return tf.concat(axis=3, values=[branch_0, branch_1, branch_2, branch_3])\n\n\ndef inception_v4_base(inputs, final_endpoint='Mixed_7d', scope=None):\n  \"\"\"Creates the Inception V4 network up to the given final endpoint.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    final_endpoint: specifies the endpoint to construct the network up to.\n      It can be one of [ 'Conv2d_1a_3x3', 'Conv2d_2a_3x3', 'Conv2d_2b_3x3',\n      'Mixed_3a', 'Mixed_4a', 'Mixed_5a', 'Mixed_5b', 'Mixed_5c', 'Mixed_5d',\n      'Mixed_5e', 'Mixed_6a', 'Mixed_6b', 'Mixed_6c', 'Mixed_6d', 'Mixed_6e',\n      'Mixed_6f', 'Mixed_6g', 'Mixed_6h', 'Mixed_7a', 'Mixed_7b', 'Mixed_7c',\n      'Mixed_7d']\n    scope: Optional compat.v1.variable_scope.\n\n  Returns:\n    logits: the logits outputs of the model.\n    end_points: the set of end_points from the inception model.\n\n  Raises:\n    ValueError: if final_endpoint is not set to one of the predefined values,\n  \"\"\"\n  end_points = {}\n\n  def add_and_check_final(name, net):\n    end_points[name] = net\n    return name == final_endpoint\n\n  with tf.compat.v1.variable_scope(scope, 'InceptionV4', [inputs]):\n    with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                        stride=1, padding='SAME'):\n      # 299 x 299 x 3\n      net = slim.conv2d(inputs, 32, [3, 3], stride=2,\n                        padding='VALID', scope='Conv2d_1a_3x3')\n      if add_and_check_final('Conv2d_1a_3x3', net): return net, end_points\n      # 149 x 149 x 32\n      net = slim.conv2d(net, 32, [3, 3], padding='VALID',\n                        scope='Conv2d_2a_3x3')\n      if add_and_check_final('Conv2d_2a_3x3', net): return net, end_points\n      # 147 x 147 x 32\n      net = slim.conv2d(net, 64, [3, 3], scope='Conv2d_2b_3x3')\n      if add_and_check_final('Conv2d_2b_3x3', net): return net, end_points\n      # 147 x 147 x 64\n      with tf.compat.v1.variable_scope('Mixed_3a'):\n        with tf.compat.v1.variable_scope('Branch_0'):\n          branch_0 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n                                     scope='MaxPool_0a_3x3')\n        with tf.compat.v1.variable_scope('Branch_1'):\n          branch_1 = slim.conv2d(net, 96, [3, 3], stride=2, padding='VALID',\n                                 scope='Conv2d_0a_3x3')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final('Mixed_3a', net): return net, end_points\n\n      # 73 x 73 x 160\n      with tf.compat.v1.variable_scope('Mixed_4a'):\n        with tf.compat.v1.variable_scope('Branch_0'):\n          branch_0 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n          branch_0 = slim.conv2d(branch_0, 96, [3, 3], padding='VALID',\n                                 scope='Conv2d_1a_3x3')\n        with tf.compat.v1.variable_scope('Branch_1'):\n          branch_1 = slim.conv2d(net, 64, [1, 1], scope='Conv2d_0a_1x1')\n          branch_1 = slim.conv2d(branch_1, 64, [1, 7], scope='Conv2d_0b_1x7')\n          branch_1 = slim.conv2d(branch_1, 64, [7, 1], scope='Conv2d_0c_7x1')\n          branch_1 = slim.conv2d(branch_1, 96, [3, 3], padding='VALID',\n                                 scope='Conv2d_1a_3x3')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final('Mixed_4a', net): return net, end_points\n\n      # 71 x 71 x 192\n      with tf.compat.v1.variable_scope('Mixed_5a'):\n        with tf.compat.v1.variable_scope('Branch_0'):\n          branch_0 = slim.conv2d(net, 192, [3, 3], stride=2, padding='VALID',\n                                 scope='Conv2d_1a_3x3')\n        with tf.compat.v1.variable_scope('Branch_1'):\n          branch_1 = slim.max_pool2d(net, [3, 3], stride=2, padding='VALID',\n                                     scope='MaxPool_1a_3x3')\n        net = tf.concat(axis=3, values=[branch_0, branch_1])\n        if add_and_check_final('Mixed_5a', net): return net, end_points\n\n      # 35 x 35 x 384\n      # 4 x Inception-A blocks\n      for idx in range(4):\n        block_scope = 'Mixed_5' + chr(ord('b') + idx)\n        net = block_inception_a(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 35 x 35 x 384\n      # Reduction-A block\n      net = block_reduction_a(net, 'Mixed_6a')\n      if add_and_check_final('Mixed_6a', net): return net, end_points\n\n      # 17 x 17 x 1024\n      # 7 x Inception-B blocks\n      for idx in range(7):\n        block_scope = 'Mixed_6' + chr(ord('b') + idx)\n        net = block_inception_b(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n\n      # 17 x 17 x 1024\n      # Reduction-B block\n      net = block_reduction_b(net, 'Mixed_7a')\n      if add_and_check_final('Mixed_7a', net): return net, end_points\n\n      # 8 x 8 x 1536\n      # 3 x Inception-C blocks\n      for idx in range(3):\n        block_scope = 'Mixed_7' + chr(ord('b') + idx)\n        net = block_inception_c(net, block_scope)\n        if add_and_check_final(block_scope, net): return net, end_points\n  raise ValueError('Unknown final endpoint %s' % final_endpoint)\n\n\ndef inception_v4(inputs, num_classes=1001, is_training=True,\n                 dropout_keep_prob=0.8,\n                 reuse=None,\n                 scope='InceptionV4',\n                 create_aux_logits=True):\n  \"\"\"Creates the Inception V4 model.\n\n  Args:\n    inputs: a 4-D tensor of size [batch_size, height, width, 3].\n    num_classes: number of predicted classes. If 0 or None, the logits layer\n      is omitted and the input features to the logits layer (before dropout)\n      are returned instead.\n    is_training: whether is training or not.\n    dropout_keep_prob: float, the fraction to keep before final layer.\n    reuse: whether or not the network and its variables should be reused. To be\n      able to reuse 'scope' must be given.\n    scope: Optional compat.v1.variable_scope.\n    create_aux_logits: Whether to include the auxiliary logits.\n\n  Returns:\n    net: a Tensor with the logits (pre-softmax activations) if num_classes\n      is a non-zero integer, or the non-dropped input to the logits layer\n      if num_classes is 0 or None.\n    end_points: the set of end_points from the inception model.\n  \"\"\"\n  end_points = {}\n  with tf.compat.v1.variable_scope(\n      scope, 'InceptionV4', [inputs], reuse=reuse) as scope:\n    with slim.arg_scope([slim.batch_norm, slim.dropout],\n                        is_training=is_training):\n      net, end_points = inception_v4_base(inputs, scope=scope)\n\n      with slim.arg_scope([slim.conv2d, slim.max_pool2d, slim.avg_pool2d],\n                          stride=1, padding='SAME'):\n        # Auxiliary Head logits\n        if create_aux_logits and num_classes:\n          with tf.compat.v1.variable_scope('AuxLogits'):\n            # 17 x 17 x 1024\n            aux_logits = end_points['Mixed_6h']\n            aux_logits = slim.avg_pool2d(aux_logits, [5, 5], stride=3,\n                                         padding='VALID',\n                                         scope='AvgPool_1a_5x5')\n            aux_logits = slim.conv2d(aux_logits, 128, [1, 1],\n                                     scope='Conv2d_1b_1x1')\n            aux_logits = slim.conv2d(aux_logits, 768,\n                                     aux_logits.get_shape()[1:3],\n                                     padding='VALID', scope='Conv2d_2a')\n            aux_logits = slim.flatten(aux_logits)\n            aux_logits = slim.fully_connected(aux_logits, num_classes,\n                                              activation_fn=None,\n                                              scope='Aux_logits')\n            end_points['AuxLogits'] = aux_logits\n\n        # Final pooling and prediction\n        # TODO(sguada,arnoegw): Consider adding a parameter global_pool which\n        # can be set to False to disable pooling here (as in resnet_*()).\n        with tf.compat.v1.variable_scope('Logits'):\n          # 8 x 8 x 1536\n          kernel_size = net.get_shape()[1:3]\n          if kernel_size.is_fully_defined():\n            net = slim.avg_pool2d(net, kernel_size, padding='VALID',\n                                  scope='AvgPool_1a')\n          else:\n            net = tf.reduce_mean(\n                input_tensor=net,\n                axis=[1, 2],\n                keepdims=True,\n                name='global_pool')\n          end_points['global_pool'] = net\n          if not num_classes:\n            return net, end_points\n          # 1 x 1 x 1536\n          net = slim.dropout(net, dropout_keep_prob, scope='Dropout_1b')\n          net = slim.flatten(net, scope='PreLogitsFlatten')\n          end_points['PreLogitsFlatten'] = net\n          # 1536\n          logits = slim.fully_connected(net, num_classes, activation_fn=None,\n                                        scope='Logits')\n          end_points['Logits'] = logits\n          end_points['Predictions'] = tf.nn.softmax(logits, name='Predictions')\n    return logits, end_points\ninception_v4.default_image_size = 299\n\n\ninception_v4_arg_scope = inception_arg_scope","metadata":{"execution":{"iopub.status.busy":"2024-05-18T14:26:56.729886Z","iopub.execute_input":"2024-05-18T14:26:56.730313Z","iopub.status.idle":"2024-05-18T14:26:56.806106Z","shell.execute_reply.started":"2024-05-18T14:26:56.730285Z","shell.execute_reply":"2024-05-18T14:26:56.804898Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error\n\n# InceptionV4 Keras implementasyonu\nfrom keras.models import Model\nfrom tensorflow.keras.layers import concatenate\nfrom keras.layers import Conv2D, MaxPool2D, Input, GlobalAveragePooling2D, AveragePooling2D, Dense, Dropout, Activation, BatchNormalization\n\ndef conv2d_with_Batch(prev_layer, nbr_kernels, filter_size, strides=(1, 1), padding='valid'):\n    x = Conv2D(filters=nbr_kernels, kernel_size=filter_size, strides=strides, padding=padding)(prev_layer)\n    x = BatchNormalization()(x)\n    x = Activation(activation='relu')(x)\n    return x\n\ndef stemBlock(prev_layer):\n    x = conv2d_with_Batch(prev_layer, nbr_kernels=32, filter_size=(3, 3), strides=(2, 2))\n    x = conv2d_with_Batch(x, nbr_kernels=32, filter_size=(3, 3))\n    x = conv2d_with_Batch(x, nbr_kernels=64, filter_size=(3, 3))\n\n    x_1 = conv2d_with_Batch(x, nbr_kernels=96, filter_size=(3, 3), strides=(2, 2))\n    x_2 = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(x)\n\n    x = concatenate([x_1, x_2], axis=3)\n\n    x_1 = conv2d_with_Batch(x, nbr_kernels=64, filter_size=(1, 1))\n    x_1 = conv2d_with_Batch(x_1, nbr_kernels=64, filter_size=(1, 7), padding='same')\n    x_1 = conv2d_with_Batch(x_1, nbr_kernels=64, filter_size=(7, 1), padding='same')\n    x_1 = conv2d_with_Batch(x_1, nbr_kernels=96, filter_size=(3, 3))\n\n    x_2 = conv2d_with_Batch(x, nbr_kernels=96, filter_size=(1, 1))\n    x_2 = conv2d_with_Batch(x_2, nbr_kernels=96, filter_size=(3, 3))\n\n    x = concatenate([x_1, x_2], axis=3)\n\n    x_1 = conv2d_with_Batch(x, nbr_kernels=192, filter_size=(3, 3), strides=2)\n    x_2 = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(x)\n\n    x = concatenate([x_1, x_2], axis=3)\n\n    return x\n\ndef reduction_A_Block(prev_layer):\n    x_1 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=192, filter_size=(1, 1))\n    x_1 = conv2d_with_Batch(prev_layer=x_1, nbr_kernels=224, filter_size=(3, 3), padding='same')\n    x_1 = conv2d_with_Batch(prev_layer=x_1, nbr_kernels=256, filter_size=(3, 3), strides=(2, 2))\n\n    x_2 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=384, filter_size=(3, 3), strides=(2, 2))\n\n    x_3 = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(prev_layer)\n\n    x = concatenate([x_1, x_2, x_3], axis=3)\n\n    return x\n\ndef reduction_B_Block(prev_layer):\n    x_1 = MaxPool2D(pool_size=(3, 3), strides=(2, 2))(prev_layer)\n\n    x_2 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=192, filter_size=(1, 1))\n    x_2 = conv2d_with_Batch(prev_layer=x_2, nbr_kernels=192, filter_size=(3, 3), strides=(2, 2))\n\n    x_3 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=256, filter_size=(1, 1))\n    x_3 = conv2d_with_Batch(prev_layer=x_3, nbr_kernels=256, filter_size=(1, 7), padding='same')\n    x_3 = conv2d_with_Batch(prev_layer=x_3, nbr_kernels=320, filter_size=(7, 1), padding='same')\n    x_3 = conv2d_with_Batch(prev_layer=x_3, nbr_kernels=320, filter_size=(3, 3), strides=(2, 2))\n\n    x = concatenate([x_1, x_2, x_3], axis=3)\n    return x\n\ndef InceptionBlock_A(prev_layer):\n    x_1 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=64, filter_size=(1, 1))\n    x_1 = conv2d_with_Batch(prev_layer=x_1, nbr_kernels=96, filter_size=(3, 3), strides=(1, 1), padding='same')\n    x_1 = conv2d_with_Batch(prev_layer=x_1, nbr_kernels=96, filter_size=(3, 3), strides=(1, 1), padding='same')\n\n    x_2 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=64, filter_size=(1, 1))\n    x_2 = conv2d_with_Batch(prev_layer=x_2, nbr_kernels=96, filter_size=(3, 3), padding='same')\n\n    x_3 = AveragePooling2D(pool_size=(3, 3), strides=1, padding='same')(prev_layer)\n    x_3 = conv2d_with_Batch(prev_layer=x_3, nbr_kernels=96, filter_size=(1, 1), padding='same')\n\n    x_4 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=96, filter_size=(1, 1))\n\n    output = concatenate([x_1, x_2, x_3, x_4], axis=3)\n    return output\n\ndef InceptionBlock_B(prev_layer):\n    x_1 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=192, filter_size=(1, 1))\n    x_1 = conv2d_with_Batch(prev_layer=x_1, nbr_kernels=192, filter_size=(7, 1), padding='same')\n    x_1 = conv2d_with_Batch(prev_layer=x_1, nbr_kernels=224, filter_size=(1, 7), padding='same')\n    x_1 = conv2d_with_Batch(prev_layer=x_1, nbr_kernels=224, filter_size=(7, 1), padding='same')\n    x_1 = conv2d_with_Batch(prev_layer=x_1, nbr_kernels=256, filter_size=(1, 7), padding='same')\n\n    x_2 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=192, filter_size=(1, 1))\n    x_2 = conv2d_with_Batch(prev_layer=x_2, nbr_kernels=224, filter_size=(1, 7), padding='same')\n    x_2 = conv2d_with_Batch(prev_layer=x_2, nbr_kernels=256, filter_size=(7, 1), padding='same')\n\n    x_3 = AveragePooling2D(pool_size=(3, 3), strides=1, padding='same')(prev_layer)\n    x_3 = conv2d_with_Batch(prev_layer=x_3, nbr_kernels=128, filter_size=(1, 1))\n\n    x_4 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=384, filter_size=(1, 1))\n\n    output = concatenate([x_1, x_2, x_3, x_4], axis=3)\n    return output\n\ndef InceptionBlock_C(prev_layer):\n    x_1 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=384, filter_size=(1, 1))\n    x_1 = conv2d_with_Batch(prev_layer=x_1, nbr_kernels=448, filter_size=(3, 1), padding='same')\n    x_1 = conv2d_with_Batch(prev_layer=x_1, nbr_kernels=512, filter_size=(1, 3), padding='same')\n    x_1_1 = conv2d_with_Batch(prev_layer=x_1, nbr_kernels=256, filter_size=(1, 3), padding='same')\n    x_1_2 = conv2d_with_Batch(prev_layer=x_1, nbr_kernels=256, filter_size=(3, 1), padding='same')\n    x_1 = concatenate([x_1_1, x_1_2], axis=3)\n\n    x_2 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=384, filter_size=(1, 1))\n    x_2_1 = conv2d_with_Batch(prev_layer=x_2, nbr_kernels=256, filter_size=(1, 3), padding='same')\n    x_2_2 = conv2d_with_Batch(prev_layer=x_2, nbr_kernels=256, filter_size=(3, 1), padding='same')\n    x_2 = concatenate([x_2_1, x_2_2], axis=3)\n\n    x_3 = MaxPool2D(pool_size=(3, 3), strides=1, padding='same')(prev_layer)\n    x_3 = conv2d_with_Batch(prev_layer=x_3, nbr_kernels=256, filter_size=(1, 1), padding='same')\n\n    x_4 = conv2d_with_Batch(prev_layer=prev_layer, nbr_kernels=256, filter_size=(1, 1))\n\n    output = concatenate([x_1, x_2, x_3, x_4], axis=3)\n    return output\n\ndef InceptionV4():\n    input_layer = Input(shape=(299, 299, 3))\n\n    x = stemBlock(prev_layer=input_layer)\n\n    x = InceptionBlock_A(prev_layer=x)\n    x = InceptionBlock_A(prev_layer=x)\n    x = InceptionBlock_A(prev_layer=x)\n    x = InceptionBlock_A(prev_layer=x)\n\n    x = reduction_A_Block(prev_layer=x)\n\n    x = InceptionBlock_B(prev_layer=x)\n    x = InceptionBlock_B(prev_layer=x)\n    x = InceptionBlock_B(prev_layer=x)\n    x = InceptionBlock_B(prev_layer=x)\n    x = InceptionBlock_B(prev_layer=x)\n    x = InceptionBlock_B(prev_layer=x)\n    x = InceptionBlock_B(prev_layer=x)\n\n    x = reduction_B_Block(prev_layer=x)\n\n    x = InceptionBlock_C(prev_layer=x)\n    x = InceptionBlock_C(prev_layer=x)\n    x = InceptionBlock_C(prev_layer=x)\n\n    # Özel bir sınıflandırıcı ekle\n    x = GlobalAveragePooling2D()(x)\n    x = Dense(units=1024, activation='relu')(x)\n    predictions = Dense(1)(x)  # Tek bir çıktı nöronu (yaş tahmini)\n\n    model = Model(inputs=input_layer, outputs=predictions, name='Inception-V4')\n\n    return model\n\n# --- Ana Kod ---\n\n# GPU Ayarları\nphysical_devices = tf.config.list_physical_devices('GPU')\nif physical_devices:\n    try:\n        # Her bir GPU için bellek büyümesini ayarla\n        for gpu in physical_devices:\n            print()\n            # tf.config.experimental.set_memory_growth(gpu, True)\n        print(\"GPU'lar kullanılıyor.\")\n    except RuntimeError as e:\n        print(f\"GPU ayarlama hatası: {e}\")\nelse:\n    print(\"GPU bulunamadı. CPU kullanılacak.\")\n\n# Veri seti yolu (Ana klasör yolu)\nveri_seti_yolu = \"/kaggle/input/miniddsm/Mini_DDSM_Upload\"\n\n# Alt klasörler\nalt_klasorler = [\"Benign\", \"Cancer\", \"Normal\"]\n\n# Tüm verileri bir listede birleştir\ntum_veriler = []\n\n# Her alt klasör için veriyi yükle ve birleştir\nfor alt_klasor in alt_klasorler:\n    excel_yolu = os.path.join(veri_seti_yolu, f\"List_{alt_klasor}.xlsx\")\n    df = pd.read_excel(excel_yolu)\n    # Görüntü yollarını oluştururken alt klasörü de ekleyin\n    df['imagePath'] = df['fileName'].apply(lambda x: os.path.join(veri_seti_yolu, alt_klasor, x))\n    tum_veriler.append(df)\n\n# Tüm veri çerçevelerini birleştir\ndf = pd.concat(tum_veriler, ignore_index=True)\n\n# Veriyi eğitim ve test kümelerine ayır\ntrain_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n\n# Görüntü boyutları\nimg_width, img_height = 220, 360  # InceptionV4 için giriş boyutu\n\n# InceptionV4 modelini oluştur\nmodel = InceptionV4()\n\n# Modelin son katmanlarını eğitmek için diğer katmanları dondur\nfor layer in model.layers[:-2]:  # Son iki katman hariç\n    layer.trainable = False\n\n# Modeli derle\nmodel.compile(optimizer='adam', loss='mean_absolute_error')\n\n# Veri artırma\ntrain_datagen = ImageDataGenerator(\n    preprocessing_function=preprocess_input,  # InceptionV4 için gerekli önişleme\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True)\n\ntest_datagen = ImageDataGenerator(preprocessing_function=preprocess_input)\n\n# Veri yükleyicilerini oluştur\ntrain_generator = train_datagen.flow_from_dataframe(\n    dataframe=train_df,\n    x_col='imagePath',\n    y_col='Age',\n    target_size=(img_width, img_height),\n    batch_size=32,\n    class_mode='raw')  # Çıktı sürekli bir değer olduğu için \"raw\" kullanılır\n\ntest_generator = test_datagen.flow_from_dataframe(\n    dataframe=test_df,\n    x_col='imagePath',\n    y_col='Age',\n    target_size=(img_width, img_height),\n    batch_size=32,\n    class_mode='raw')\n\n# Modeli eğit (RAM kullanımını azaltmak için küçük bir epoch sayısı)\nmodel.fit(\n    train_generator,\n    epochs=50,\n    validation_data=test_generator)\n\n# Özellik vektörlerini al\nfeatures = model.predict(test_generator)\n\n# KNN regresörü\nknn = KNeighborsRegressor(n_neighbors=5)\nknn.fit(features, test_df['Age'])\n\n# Tahminler\nknn_predictions = knn.predict(features)\n \n# Performans ölçümleri\nmae = mean_absolute_error(test_df['Age'], knn_predictions)\nmse = mean_squared_error(test_df['Age'], knn_predictions)\n\nprint(f\"Ortalama Mutlak Hata (MAE): {mae}\")\nprint(f\"Karesel Ortalama Hata (MSE): {mse}\")\n\n# Yeni bir resim için yaş tahmini yapma fonksiyonu\ndef yas_tahmin_et(resim_yolu):\n    # Resmi yükle ve önişle\n    img = load_img(resim_yolu, target_size=(img_width, img_height))\n    x = img_to_array(img)\n    x = np.expand_dims(x, axis=0)\n    x = preprocess_input(x)  # InceptionV4 için gerekli önişleme\n       \n    # Özellik vektörünü al\n    features = model.predict(x)\n\n    # Yaş tahmini yap\n    yas_tahmini = knn.predict(features)\n\n    return yas_tahmini[0]\n \n# Tahmin edilecek resmin yolu\ntest_resim_yolu = \"/kaggle/input/miniddsm/Mini_DDSM_Upload/Normal/1000_A_0358_1.RIGHT_CC.LJPEG.1_highpass.png\"  # Örnek resim yolu\n\n# Yaş tahminini yap\ntahmin_edilen_yas = yas_tahmin_et(test_resim_yolu)\n\nprint(f\"Tahmin Edilen Yaş: {tahmin_edilen_yas:.0f}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-18T14:35:07.241292Z","iopub.execute_input":"2024-05-18T14:35:07.241855Z","iopub.status.idle":"2024-05-18T17:56:24.338623Z","shell.execute_reply.started":"2024-05-18T14:35:07.241805Z","shell.execute_reply":"2024-05-18T17:56:24.337464Z"},"trusted":true},"execution_count":47,"outputs":[{"name":"stdout","text":"\n\nGPU'lar kullanılıyor.\nFound 7747 validated image filenames.\nFound 1937 validated image filenames.\nEpoch 1/50\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:120: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n  self._warn_if_super_not_called()\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1716042987.568979     640 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\nW0000 00:00:1716042987.694101     640 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m127/243\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2:11\u001b[0m 1s/step - loss: 56.6656","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1716043130.800038     639 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1s/step - loss: 53.1448","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1716043255.653115     637 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m355s\u001b[0m 1s/step - loss: 53.1054 - val_loss: 15.4242\nEpoch 2/50\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1716043290.841217     639 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 945ms/step - loss: 12.0340 - val_loss: 10.6893\nEpoch 3/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 952ms/step - loss: 10.8976 - val_loss: 10.6801\nEpoch 4/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 946ms/step - loss: 10.9443 - val_loss: 10.6857\nEpoch 5/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m233s\u001b[0m 932ms/step - loss: 10.8740 - val_loss: 10.6558\nEpoch 6/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 941ms/step - loss: 11.1274 - val_loss: 10.6565\nEpoch 7/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 951ms/step - loss: 10.8004 - val_loss: 10.6272\nEpoch 8/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 956ms/step - loss: 10.8202 - val_loss: 10.6182\nEpoch 9/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 948ms/step - loss: 10.8321 - val_loss: 10.5949\nEpoch 10/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 952ms/step - loss: 10.8634 - val_loss: 10.5920\nEpoch 11/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 960ms/step - loss: 10.8455 - val_loss: 10.5694\nEpoch 12/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 951ms/step - loss: 10.7350 - val_loss: 10.5530\nEpoch 13/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 952ms/step - loss: 10.8476 - val_loss: 10.5360\nEpoch 14/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 955ms/step - loss: 10.8720 - val_loss: 10.5264\nEpoch 15/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 952ms/step - loss: 10.8331 - val_loss: 10.5048\nEpoch 16/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 950ms/step - loss: 10.5496 - val_loss: 10.4959\nEpoch 17/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 952ms/step - loss: 10.7946 - val_loss: 10.5067\nEpoch 18/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 952ms/step - loss: 10.7957 - val_loss: 10.4721\nEpoch 19/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 947ms/step - loss: 10.8694 - val_loss: 10.4669\nEpoch 20/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 951ms/step - loss: 10.6668 - val_loss: 10.4881\nEpoch 21/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 948ms/step - loss: 10.5914 - val_loss: 10.4377\nEpoch 22/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 954ms/step - loss: 10.6017 - val_loss: 10.5064\nEpoch 23/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 952ms/step - loss: 10.6131 - val_loss: 10.4178\nEpoch 24/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 954ms/step - loss: 10.5543 - val_loss: 10.4422\nEpoch 25/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 946ms/step - loss: 10.6011 - val_loss: 10.3969\nEpoch 26/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 952ms/step - loss: 10.5170 - val_loss: 10.3888\nEpoch 27/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 949ms/step - loss: 10.7315 - val_loss: 10.3878\nEpoch 28/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 954ms/step - loss: 10.5884 - val_loss: 10.3724\nEpoch 29/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 948ms/step - loss: 10.7722 - val_loss: 10.3729\nEpoch 30/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 948ms/step - loss: 10.5285 - val_loss: 10.3713\nEpoch 31/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m263s\u001b[0m 948ms/step - loss: 10.5055 - val_loss: 10.3640\nEpoch 32/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 952ms/step - loss: 10.6695 - val_loss: 10.3475\nEpoch 33/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 952ms/step - loss: 10.6374 - val_loss: 10.4216\nEpoch 34/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 952ms/step - loss: 10.6472 - val_loss: 10.3387\nEpoch 35/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 937ms/step - loss: 10.5305 - val_loss: 10.3300\nEpoch 36/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m234s\u001b[0m 936ms/step - loss: 10.6302 - val_loss: 10.3325\nEpoch 37/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 952ms/step - loss: 10.5476 - val_loss: 10.3316\nEpoch 38/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m237s\u001b[0m 949ms/step - loss: 10.6811 - val_loss: 10.3260\nEpoch 39/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 946ms/step - loss: 10.6524 - val_loss: 10.3343\nEpoch 40/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 953ms/step - loss: 10.5926 - val_loss: 10.3146\nEpoch 41/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 955ms/step - loss: 10.5970 - val_loss: 10.3102\nEpoch 42/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 950ms/step - loss: 10.5030 - val_loss: 10.3078\nEpoch 43/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 946ms/step - loss: 10.5782 - val_loss: 10.3120\nEpoch 44/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 945ms/step - loss: 10.4744 - val_loss: 10.2985\nEpoch 45/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 943ms/step - loss: 10.4598 - val_loss: 10.3385\nEpoch 46/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 945ms/step - loss: 10.4651 - val_loss: 10.3596\nEpoch 47/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 946ms/step - loss: 10.6275 - val_loss: 10.2925\nEpoch 48/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m238s\u001b[0m 953ms/step - loss: 10.6361 - val_loss: 10.3354\nEpoch 49/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m239s\u001b[0m 955ms/step - loss: 10.4273 - val_loss: 10.2882\nEpoch 50/50\n\u001b[1m243/243\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m236s\u001b[0m 943ms/step - loss: 10.6912 - val_loss: 10.2876\n\u001b[1m 1/61\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10:24\u001b[0m 10s/step","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1716054946.306038     637 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m61/61\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 390ms/step\n","output_type":"stream"},{"name":"stderr","text":"W0000 00:00:1716054969.721859     637 graph_launch.cc:671] Fallback to op-by-op mode because memset node breaks graph update\n","output_type":"stream"},{"name":"stdout","text":"Ortalama Mutlak Hata (MAE): 9.19659266907589\nKaresel Ortalama Hata (MSE): 125.16173464119773\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 14s/step\nTahmin Edilen Yaş: 62\n","output_type":"stream"}]}]}